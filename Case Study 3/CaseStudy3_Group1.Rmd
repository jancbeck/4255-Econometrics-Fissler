---
title: "Case Study 3 - Group 1"
author:
- Annika Janson h11829506
- Jan Beck h11814291
date: "10.5.2021"
output:
  pdf_document: default
  html_document:
    df.print: paged
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \renewcommand{\and}{\\}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
#library(car)
#library(forecast)
library(ggplot2)
library(stargazer)
library(MASS)


knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
babies <- read.csv("babies.csv")
fish <- read.csv("fish.csv")
fish$year <- as.factor(fish$year)
```

# 1.1

```{r}
fish_poisson1 <- glm(intensity ~  area + year + depth + length + sex + age,
               data=fish, family = "poisson")

#summary(fish_poisson1)

# add non-linearity for length
fish_poisson2 <- glm(intensity ~  area + year + depth + length + sex + age + I(length^2),
               data=fish, family = "poisson")

#summary(fish_poisson2)

# add non-linearity for age
fish_poisson3 <- glm(intensity ~  area + year + depth + length + sex + age + I(age^2),
               data=fish, family = "poisson")

#summary(fish_poisson3)

# add non-linearity for depth
fish_poisson4 <- glm(intensity ~  area + year + depth + length + sex + age + I(depth^2),
               data=fish, family = "poisson")

#summary(fish_poisson4)

# add interaction term for sex*length
fish_poisson5 <- glm(intensity ~  area + year + depth + length + sex + age + I(sex*length),
               data=fish, family = "poisson")

#summary(fish_poisson5)

# add interaction term for sex*depth
fish_poisson6 <- glm(intensity ~  area + year + depth + length + sex + age + I(sex*depth),
               data=fish, family = "poisson")

#summary(fish_poisson6)

# add interaction term for sex*age
fish_poisson7 <- glm(intensity ~  area + year + depth + length + sex + age + I(sex*age),
               data=fish, family = "poisson")

#summary(fish_poisson7)

# combine model 4 and 6
fish_poisson8 <- glm(intensity ~  area + year + depth + length + sex + age + I(depth^2) + I(sex*depth),
               data=fish, family = "poisson")

#summary(fish_poisson8)

# combine model 8 and 2
fish_poisson9 <- glm(intensity ~  area + year + depth  + length + sex + age+ I(depth^2) + I(sex*depth)+ I(length^2),
               data=fish, family = "poisson")

#summary(fish_poisson10)

# combine model 9 and 3
fish_poisson10 <- glm(intensity ~  area + year + depth + length + sex + age + I(depth^2) + I(sex*depth)+ I(length^2) + I(age^2),
               data=fish, family = "poisson")

#summary(fish_poisson11)

# combine model 10 and 3
fish_poisson11 <- glm(intensity ~  area + year + depth + length + sex + age + I(depth^2) + I(sex*depth)+ I(length^2) + I(age^2) + I(sex*age),
               data=fish, family = "poisson")

#summary(fish_poisson11)

# combine model 11 and 3
fish_poisson12 <- glm(intensity ~  area + year + depth + length + sex + age + I(depth^2) + I(sex*depth)+ I(length^2) + I(age^2) + I(sex*age) + I(sex*length),
               data=fish, family = "poisson")

#summary(fish_poisson12)


```

### 1.1.1

On page 2, we report the fitted Poisson model in column 1. All
independent variables were significant at the $\alpha=0.01$ level except
for the areas "Soroya" and "Tanafjord". The AIC of the model is
`r format(AIC(fish_poisson1))` and the BIC is
`r format(BIC(fish_poisson1))`.

### 1.1.2

First, we added quadratic terms of `length`, `age` and `depth` to model 1
(from 1.1.1) and got models 2, 3, 4 respectively. We then added
interaction terms of variable `sex` with `length`, `depth` and `age` and got models 5, 6 and 7 respectively. The coefficients of
these models can be found on page 2.

We then combined model 4 and 6, because these had the lowest BIC and AIC
and got model 8 which had an even lower BIC/AIC. We then progressively
added the interaction and non-linear terms from models 2, 3, 5, 7. These resulting models are shown on
page 3. The AIC/BICs of our 11 models are reported in table 1.

```{r, results='asis', echo=FALSE}

fish_AIC <- c(AIC(fish_poisson1), AIC(fish_poisson2), AIC(fish_poisson3), AIC(fish_poisson4), AIC(fish_poisson5), AIC(fish_poisson6), AIC(fish_poisson7), AIC(fish_poisson8), AIC(fish_poisson9), AIC(fish_poisson10), AIC(fish_poisson11), AIC(fish_poisson12))
fish_BIC <- c(BIC(fish_poisson1), BIC(fish_poisson2), BIC(fish_poisson3), BIC(fish_poisson4), BIC(fish_poisson5), BIC(fish_poisson6), BIC(fish_poisson7), BIC(fish_poisson8), BIC(fish_poisson9), BIC(fish_poisson10), BIC(fish_poisson11), BIC(fish_poisson12))
fish_criteria <- cbind(c(1:12), fish_AIC, fish_BIC)

knitr::kable(
  fish_criteria, caption = 'Model selection criteria of our Poisson models.'
)

```

```{=tex}
\newpage
\blandscape
```

```{r, results='asis', echo=FALSE}
invisible(stargazer(fish_poisson1, fish_poisson2, fish_poisson3, fish_poisson4, fish_poisson5, fish_poisson6, fish_poisson7, header=FALSE, type='latex', title="Poisson models", align=TRUE, report="vc*", column.sep.width="0pt"))

invisible(stargazer(fish_poisson1, fish_poisson8, fish_poisson9, fish_poisson10, fish_poisson11, fish_poisson12, header=FALSE, type='latex', title="Combined Poisson models", align=TRUE, report="vc*", column.labels=c("(1)", "(8)", "(9)", "(10)", "(11)", (12)), colnames=FALSE, model.numbers=FALSE))

```

\elandscape

### 1.1.3

The best model we found in 1.1.2 in terms of model selection criteria had an AIC of `r format(AIC(fish_poisson11))` and BIC of `r format(BIC(fish_poisson11))` and is shown in column (12) on page 3.
All of its predictors are statistically significant at the $\alpha=0.05$ level except for the year2001, age and age^2.

We interpret the coefficients as the expected difference in the total log count of expected parasites of a one unit change when holding all other variables constant. Or in mathematical terms:

$$\frac{\mathbb{E}\left(Y \mid X_{j}=1, \ldots\right)-\mathbb{E}\left(Y \mid X_{j}=0, \ldots\right)}{\mathbb{E}\left(Y \mid X_{j}=0, \ldots\right)}=e^{\beta_{j}}-1\approx\beta_{j}$$
For small $\beta_j$, the relative change is roughly equal to $100*\beta_j$ percent.

**Intercept/Constant:** The expected log count of parasites of a 1 year old male fish of size 0 caught in 1999 in the "Mageroya" area at sea level. 

area**soroya:** Fish caught in this area has on average slightly less parasites than in the Mageroya area c.p., because the sign of the coefficient is negative.

area**tanafjord:** Fish caught in this area has on average less parasites than in the Mageroya area c.p., because the sign of the coefficient is negative.

area**varangerfjord:** Fish caught in this area has on average more parasites than in the Mageroya area c.p., because the sign of the coefficient is positive.

**year2000/year2001:** Because of the signs of these coefficients, all else equal, the expected log count of number of parasites is higher/lower in 2000/2001 than in 1999.

**depth + I(depth\^2):** The expected log count of parasites increases quadratically with the depth in which a fish is caught up to a certain depth before it decreases (the apex lies within the sample values), c.p.

**length+ I(length\^2):** The expected log count of parasites decreases quadratically with the length of a fish (the apex lies outside the sample values), c.p.

**age + I(age^2):** All else equal, the average log count of number of parasites increases quadratically with age. This could be because older fish have less resistance towards them and/or they have accumulated more during their lifetime.

**sex:** Because this coefficient is positive, all else equal, we expect a higher log count of number of parasites for female fish.

**I(sex * depth) + I(sex * age) + I(sex * length):** For female fish, we add this parameter to the corresponding linear coefficient i.e. $\beta_{depth} \Delta_{x_{depth}}+\beta_{} \Delta_{x_{1}} x_{2}$. If the resulting value is negative, the apex quadratic function and its apex move to right. If it's positive, they move to the left. This can change the interpretation of these coefficients, because now the apex could fall within the sample space.

\newpage

### 1.1.4

```{r}
fish_nb <- glm.nb(intensity ~  area + year + depth + length + sex + age + I(depth^2) + I(length^2) + I(age^2) + I(sex*age) + I(sex*length) + I(sex*depth), data=fish)
summary(fish_nb)
```

\newpage

The resulting AIC of our negative binomial model (NB) is `r format(AIC(fish_nb))` and the BIC `r format(AIC(fish_nb))`. That is a lot smaller than the AIC `r format(AIC(fish_poisson11))` and BIC `r format(BIC(fish_poisson11))` of our chosen Poisson model. The fact that the model selection criteria prefer the NB model so strongly, suggests that the assumption for our Poisson model that

$$\mathbb{E}(Y)=\mathbb{V}(Y)=\mu$$

is violated and that actually

$$\mathbb{V}(Y)>\mathbb{E}(Y).$$


We compute the variance and mean to confirm this:

```{r, echo=TRUE}
var(fish$intensity)
mean(fish$intensity)
```

The distribution is overdispersed and could have arisen from the negative binomial distribution:

$$\mathbb{V}(Y)=\mathbb{E}(Y)\left(1+\eta^{2} \mathbb{E}(Y)\right)$$
\newpage

# 1.2

### 1.2.1

```{r, include=FALSE}

# Linear Probability Model
babies$race <- as.factor(babies$race)

is.numeric(babies$race)
is.factor(babies$race)

LinearProbability <- lm(low ~ smoke + race + age + lwt + ptl + ht + ui + ftv, data=babies)

summary(LinearProbability)

# Binary Probit Model
probit_mod <- glm(low ~ smoke + race + age + lwt +ptl + ht + ui +ftv, family = binomial(link = "probit"), data=babies)

summary(probit_mod)

median(babies$lwt)
is.factor(babies$smoke)

# Binary Logit Model
logit_mod <- glm(low ~ smoke + race + age + lwt +ptl + ht + ui +ftv, family = binomial(link = "logit"), data=babies)

summary(logit_mod)

```

We report our Linear Probability Model (OLS), Binary Probit and Binary Logit models in table 4.

```{r, results='asis', echo=FALSE}
invisible(stargazer(LinearProbability, probit_mod, logit_mod, header=FALSE, type='latex', title="'Babies' models", align=TRUE, model.names=TRUE))
```


### 1.2.2

```{r, results='asis', echo=FALSE}

babies_AIC <- c(format(AIC(LinearProbability)), format(AIC(probit_mod)), format(AIC(logit_mod)))
babies_BIC <- c(format(BIC(LinearProbability)), format(BIC(probit_mod)), format(BIC(logit_mod)))
babies_criteria <- cbind(c("Linear Probability", "Binary Probit", "Binary Logit"), babies_AIC, babies_BIC)

knitr::kable(
  babies_criteria, caption = "Model selection criteria of our 'Babies' models."
)

```

Looking at AIC, the Binary Probit Model performs best. This is supported by the BIC as in both cases the Binary Logit Model results in the lowest value. AIC and BIC of the Logit Model differ only slightly however, so it seems almost as good. We would nonetheless choose the Logit model.

### 1.2.3

```{r}
explore_params <- function(mod, param, grouping_param, n = 100, silent = FALSE){
  
  # Its just nicer for some things
 
  
  # Extract covariates from model object
  covariates <- mod$data[, names(mod$data) %in% attr(mod$terms, "term.labels")]
  
  # Differentiate between dummy and non-dummy variables
  # It essentially just checks whether all values are either 0 or 1
  is.dummy <- apply(covariates, 2, function(x){
    all(x %in% c(0, 1))
  })
  
  # The grouping variable cant be continuous
  if (missing(grouping_param) == FALSE && is.dummy[grouping_param] == FALSE){
    stop("grouping_param has to be a dummy variable!")
  }
  
  # Dummy variables and continuous variables produce different plots, have to differentiate
  if (is.dummy[param]){
    # Generate synthetic observations for dummy = 0 and dummy = 1
    neg_synth <- pos_synth <- covariates
    
    # Overwrite all dummy variables with 0 for negative synth 
    neg_synth[,param] <- 0
    
    # Overwrite all dummy variables with 1 for positive synth
    pos_synth[, param] <- 1
    
    # Predict average probability of unemployment over synthetic observations
    pos_pred <- mean(predict(mod, pos_synth, type = "response"))
    neg_pred <- mean(predict(mod, neg_synth, type = "response"))
    
    # This is just stuff that ggplot2 wants
    data <- data.frame(Probability = c(neg_pred, pos_pred), val = as.factor(c(0, 1)))
    names(data)[2] <- param
    
    p <- ggplot(data, aes_string(y = "Probability", x = 0, fill = param)) +
      geom_bar(stat = "identity", position="dodge") + 
      ylab("Average Probability") +
      theme_bw() + 
      theme(axis.title.x=element_blank(),
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank()) +
      ylim(c(0, 1)) +
      annotate("text", label = paste0("Difference: ", round(100*(pos_pred - neg_pred), 2), "%"),  
               x = -0.25, y = 0.95, hjust = 0.5)
    
  } else {
    # Get range of predictor variable
    par_range <- range(covariates[, param])
    
    # Create n evenly spaced values in the range of the param
    seq <- seq(from = par_range[1], to = par_range[2], length.out = n)
    
    # Loop over all points in seq and create synthetic observations
    avg_probs <- rep(NA, length(seq))
    
    if (missing(grouping_param)){
      avg_probs2 <- NULL
    } else {
      avg_probs2 <- rep(NA, length(seq))
    }
    
    
    for (i in 1:length(seq)){
      curr_dat <- covariates
      
      # Replace all values of param with current value to create synthetic observations
      curr_dat[, param] <- seq[i]
      
      # Create two seperate sets of synthetic observations if grouping_param was provided
      # This lets the user look at the effect of covariates on different groups
      if (missing(grouping_param) == FALSE){
        curr_dat[, grouping_param] <- 0
        curr_dat2 <- curr_dat
        curr_dat2[, grouping_param] <- 1
        
        avg_probs2[i] <- mean(predict(mod, curr_dat2, type = "response"))
      }
      
      # Calculate average probability for all synthetic observations at current value of param
      avg_probs[i] <- mean(predict(mod, curr_dat, type = "response"))
    }
    
    # Create data frame for ggplot again
    probs <- c(avg_probs, avg_probs2)
    data <- data.frame(Probability = probs, 
                       val = rep(seq, length.out = length(probs)),
                       group = as.factor(rep(0:1, each = length(seq))[1:length(probs)]))
    
    names(data)[2] <- param
    if (missing(grouping_param) == FALSE){
      names(data)[3] <- grouping_param
    }
    
    
    p <- ggplot(data)
    if (missing(grouping_param)) {
      p <- p + geom_line(lwd = 1.2, mapping = aes_string(x = param, y = "Probability"))
    } else {
      p <- p + geom_line(lwd = 1.2, mapping = aes_string(x = param, y = "Probability", color = grouping_param)) 
    }
    
    coord_x <- min(min(data[, param]))
    coord_y <- 0.95
    if (missing(grouping_param)){
      annot <- paste0("Max difference: ", round(100*(max(avg_probs) - min(avg_probs)), 2), "%")
      
    } else {
      annot <- c(paste0("Max difference, ", grouping_param, " = 0: ", round(100*(max(avg_probs) - min(avg_probs)), 2), "%"),
                      paste0("Max difference, ", grouping_param, " = 1: ", round(100*(max(avg_probs2) - min(avg_probs2)), 2), "%"))
      
      coord_y <- c(coord_y, 0.95*coord_y)
      
    }
    
    p <- p + theme_bw() + 
      ylim(c(0, 1)) + 
      ylab("Average Probability") + 
      annotate("text", label = annot, 
               x = coord_x,
               y = coord_y, hjust = 0, vjust = 0.5)     
  }
  if (silent == FALSE){
    print(p)
  }
  return(p)
}

```

The predictors `smoke`, `race2`, `race3`, `lwt` and `ht` are significant at the $\alpha=0.05$ level in all our three models.

### Linear Probability Model:

The intercept $\beta_0$ shows us that the conditional probability that the birth weight of a baby is under 2500g is approximately __51,03%__ holding all other variables constant. 
Holding all other variables constant, the other coefficients can be interpreted as followed (significance level = 5%):

smoke: If someone smokes the probability that the birth weight is under 2500g is increasing by approximately __15,99%__ c.p.
race 2: If someone is of black colour the probability that the birth weight is under 2500g is increasing by approximately __22,15%__ (in comparison to someone of white colour) c.p.
race 3: not significant
age: age seems to be not significant
lwt: With increasing weight (lbs) of mother at last menstrual period by 1 the probability that the birth weight is under 2500g is decreasing by __0.25%__ c.p.
ptl: not significant
ht: If hypertension occurred in the past the probability that the birth weight is under 2500g is increasing by approximately __36,62%__ c.p
ui: not significant
ftv: not significant

```{r}
pnorm(0.270956)
plogis(0.477788 )


```

### Logit/ Probit model 

Conditional probability that the birth weight of a baby is under 2500g is approximately is for both models around 61%. 

Only __age__ and __lwt__ have negative signs. That means if age increases by 1 year or weight (lbs) of mother at last menstrual period increases by __1 lbs__ the probability that the birth weight of a baby is under 2500g decreases, c.p. Age is not significant however. For all other parameters the probability increases, c.p. 

### Probit/ Logit Model: 

Significant Parameters: 

\newpage
```{r}

explore_params(probit_mod, "smoke", silent = TRUE) + ggtitle("Probit model")
explore_params(logit_mod, "smoke", silent = TRUE) + ggtitle("Logit model")
```
\newpage
```{r}
explore_params(probit_mod, "lwt", silent = TRUE) + ggtitle("Probit model")
explore_params(logit_mod, "lwt", silent = TRUE) + ggtitle("Logit model")
```
\newpage
```{r}
explore_params(probit_mod, "ht", silent = TRUE) + ggtitle("Probit model")
explore_params(logit_mod, "ht", silent = TRUE) + ggtitle("Logit model")

```
\newpage

We observe a non linear effect that in both models the probability that the birth weight of a baby is under 2500g is decreasing with increasing weight (lbs) of mother at last menstrual period averaging over all of the other parameters. The difference between the person with the lowest and the highest person was on average around __37%__. 

We calculated the average probability for different types of women and can observe that the average difference in probability between a woman who is smoking and a woman who does not smoke is about __17%__. The probability that the birth weight of a baby is under 2500g is higher for a woman who is smoking.

We again calculated the average probability for different type of women and can observe that the average difference in probability between a woman with a history of hypertension and a woman without is around __37%__ for both models. While it is higher for woman with a history of hypertension that the birth weight of a baby is under 2500g.



### 1.2.4

Predictions: 
```{r}

babies_pred <- data.frame(low= 1, smoke = 0, race = as.factor(1), age = 30, lwt= 140, ptl= 0, ht= 0, ht = 1, ui = 0, ftv= 2)

babies_pred_lp <- predict(LinearProbability, babies_pred)
#babies_pred_lp

babies_pred_probit <- predict(probit_mod, babies_pred, type = "response")
#babies_pred_probit

babies_pred_logit <- predict(logit_mod, babies_pred, type = "response")
#babies_pred_logit


```

The highest probability predicted  that a baby has a birth weight under 2500kg with the given 
variables is predicted by the Binary Logit Model. With a probability of __0.08060108__, so around __8.06%__ a child is born with under 2500kg. As we observed before the Binary Probit Model fits a little bit better, so we want take a look at the predicted probability in this model. It is __0.07036926__, so around __7.03%__ and around 1% lower than the probability of the Binary Logit Model.



# 2 Theory

We would like to model the expected final mark which can take on values in the set {1,2,3,4,5}. This is a case of an ordered categorical variable, since we argue that the grades are not evenly-spaced and thus non-linear. The predictors $X_1$, $X_2$ and $X_3$ are interval variables while $X_4$ is, like Y, an ordered categorical variable.

If we used a "standard" linear regression model for this task where

$Y=\beta_{0}+\beta_{1} X_{1}+\ldots+\beta_{K} X_{K}+u, \quad \mathbb{E}\left(u \mid X_{1}, \ldots, X_{K}\right)=0$

we would run into multiple problems. First, the final mark can only take on integer values, but we would be predicting interval numbers. Second, we could have heterskedastic residuals. Third, when predicting the final grade, we could end up with negative numbers (for example, if we assumed a student who studied especially hard) or numbers larger than 5. Alternatively we could model the log of the expected grade Y, but that would still leave problem 1 and 2. We could also consider a Poisson distribution, but this would again cause us to predict values outside of the set.

This problem seems to be similar to a logistic regression, with the
difference that we have 5 different possible outcomes. Therefore we
suggest to use the **soft max function**. The softmax function is
sometimes called the multi-class logistic regression. This is because
the softmax is a generalization of logistic regression that can be used
for multi-class classification, and its formula is very similar to the
sigmoid function which is used for logistic regression. We want any
output Y as probability that a given item belongs to one of our 5
classes.

$\hat{y} = softmax(o)$ where $\hat{y_j} = exp(o_j))/(\sum_k exp(o_k))$

The softmax function gives us any vector $\hat{y}$ which can be
interpreted as the estimated conditional probability of each class k
{1,2,3,4,5} given any input (X1,X2,X3,X4,X5). So for each parameter X we
get 5 probabilities.

Bonus Question:

To model the most likely final mark we need the maximum likelihood
estimator of our soft max function. For that we calculate the set of
parameters that maximize the soft max function and gives us the highest
probability.


