---
title: "Case Study 2 - Group 1"
author:
- Annika Janson h11829506
- Jan Beck h11814291
- Franz Uchatzi h1451890
date: "19.4.2021"
output:
  pdf_document: default
  html_document:
    df.print: paged
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \renewcommand{\and}{\\}
---

```{r setup, include=FALSE}
library(car)
library(forecast)
library(apsrtable)
library(lmtest)


knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
gdp <- read.csv("USGDP.csv")
N <- nrow(gdp)
```

# 1.2 Tasks

### 1.2.1

```{r}
ts_gdp <- window( ts(gdp$NA000334Q, frequency = 4, start = c(2010, 1)), start=c(2010, 1), end=c(2019, 4) ) # get only subset till Q4 2019
md1 <- tslm(ts_gdp ~ trend + season)
md1_sum <- summary(md1)
md1_bic <- BIC(md1)
md1_aic <- AIC(md1)
md1_coef <- as.vector(coefficients(md1))
```

```{r md1,  results='asis', echo=FALSE }
apsrtable::apsrtable(md1, Sweave = TRUE, col.hspace="1em", caption="Figure 1: Model 1 summary")
```

The adjusted R^2^ is **`r md1_sum$adj.r.squared`**, the BIC is **`r md1_bic`** and the AIC is **`r md1_aic`**.

### 1.2.2

General formula for modeling trend and seasonality: $$
\begin{aligned}
{Y_{t} = \mu_t + \tilde{s}_t + u_t }
\end{aligned}
$$ This general formula describes the observation $Y_{t}$ in dependence of a seasonal effect $\tilde{s}_t$ deviating from the mean $\mu_t$ which acts as a trend level plus the error term $u_t$.

General formula of Model 1: $$
\begin{aligned}
Y_{t} = \beta_{0} + \beta_{1}t + \gamma_2 D_{t,2} + \gamma_3 D_{t,3} + \gamma_4 D_{t,4} + u_t
\end{aligned}
$$ The general formula for model 1 consists of a linear trend $\beta_{0}$ and $\beta_{1}t$ where $\beta_{0}$ is the intercept and $\beta_{1}$ is the slope of the linear trend. The seasonal effect $\tilde{s}_t$ consists of four quarter whereas quarter 1 acts as the baseline for the subsequent quarter. Therefore only quarter 2 to 4 are described by the term $\gamma_j D_{t,j}$. Explicitly, for season 2 (i.e. second quarter) the term $\gamma_2 D_{t,2}$ consists of a dummy variable $D_{t,2}$ and the deviation of the baseline $\gamma_2$. The variable $D_{t,2}$ is 1 if $t$ is in season 2 (i.e. second quarter) and 0 otherwise. Analogously, this is true for the term $\gamma_3 D_{t,3}$ and $\gamma_4 D_{t,4}$ respectively.

### 1.2.3

The intercept value of **`r format(md1_coef[1])`** means that this is the expected value of US GDP at t=0 or Q1 2010. On average, US GDP grows by **`r format(md1_coef[2])`** every quarter. In a second quarter of a year, US GDP grows additionally by **`r format(md1_coef[3])`**, **`r format(md1_coef[4])`** in a third and **`r format(md1_coef[5])`** in the last quarter of a year. The implies that the first quarter is the weakest quarter, because all seasonal variables are positive and the trend and intercept parameters act as a baseline for the first quarter.\
The precise values of the 5 estimators can also be seen in the Model 1 summary table under **1.2.1**.

### 1.2.4

```{r}
plot(ts_gdp, main = "Fig. 1: US GDP and Model 1 fitted values", ylab = "in millions of Dollars", xlab = "Year")
lines(md1$fitted.values, col="red")
legend("bottomright", legend=c("US GDP", "Fitted values"), col=c("black", "red"), lty=1:1)

plot(md1$residuals, main = "Fig. 2: Model 1 residuals", ylab = "residuals", xlab = "Year", col="red")
abline(0,0, col="blue")

```

Assumption A1, $\mathbb{E}\left(u_{t}\right)=0$, seems violated, since the residuals lie mostly above 0 for values before 2012 and after 2018 and below 0 for the values in between. This suggests there is a systematic error in our model.\
Assumption A2, $\mathbb{V}\left(u_{t}\right)=\sigma_{u}^{2}$, seems violated because the residual values tend to fluctuate stronger in the time frame of 2012 to 2014 than they to from 2014 till 2017. They therefore seem not independent.

### 1.2.5

```{r}
Acf(md1$residuals, main = "Fig. 3: ACF Model 1 residuals ")
Pacf(md1$residuals, main = "Figure 4: PACF Model 1 residuals")

```

We know that a MA-process cuts off at lag q in the ACF. Looking at the plot of our ACF, we see that autocorrelation stays relatively even, before it cuts off at lag 4. Therefore, we assume an MA-process of order 4.

We also know that an AR-process cuts off at lag p. Looking at the plot of our PACF, we can see that autocorrelation cuts off immediately. We therefore conclude that this is an AC-process of order 1.

### 1.2.6

```{r, include=FALSE}
order_checker <- function(Y, max.order = 3, xreg = NULL, I = 0, include.drift = FALSE){

  params <- list()

  for (j in c("ar", "ma")){
    for (i in 1:max.order){
     params[[paste0(j, i)]] <- c(0, NA)
    }
  }

  if (!is.null(xreg)){
    for (j in colnames(xreg)){
      params[[j]] <- NA
    }
  }

  params$intercept <- NA

  combinations <- expand.grid(params)
  res <- data.frame(BIC = rep(NA, nrow(combinations)), AIC = NA)


  pb <- txtProgressBar(1, nrow(combinations), style = 3)
  for (i in 1:nrow(combinations)){
    curr_mod <- try(Arima(Y, order = c(max.order, I, max.order), fixed = c(combinations[i, ]),
                      xreg = xreg, method = "ML", include.drift = include.drift), silent = TRUE)

    if ("try-error" %in% attr(curr_mod, "class")){
      res$BIC[i] <- NA
      res$AIC[i] <- NA
    } else {
      res$BIC[i] <- BIC(curr_mod)
      res$AIC[i] <- AIC(curr_mod)
    }
    setTxtProgressBar(pb, i)
  }

  res <- merge(res, combinations, by = 0)
  res$Row.names <- NULL
  return(res)
}

trend <- seq(from=1, to=length(ts_gdp), by=1)
season2 <- rep(c(0,1,0,0), 10)
season3 <- rep(c(0,0,1,0), 10)
season4 <- rep(c(0,1,0,1), 10)
m <- cbind(trend, season2, season3, season4)

res <- order_checker(ts_gdp, max.order = 4, xreg = m)
#res[which.min(res$BIC), ]
#res[which.min(res$AIC), ]
res_aic <- res[which.min(res$AIC),][2]
res_bic <- res[which.min(res$BIC),][1]
```

The ARMA model with the order of (1,4) results in the lowest value of BIC and AIC for all possible ARMA models up to order (4,4). The BIC is **`r format(res_bic)`** and the AIC is **`r format(res_aic)`**.

### 1.2.7

General formula Model 2: $$
\begin{aligned}
Y_{t}=\beta_{0} + \beta_{1}t + \gamma_2 D_{t,2} + \gamma_3 D_{t,3} + \gamma_4 D_{t,4}+\phi_{1}\left(Y_{t-1}-\left(\beta_{0}+\beta_{1}(t-1)\right)\right)+\theta_{1} u_{t-4}+u_{t}
\end{aligned}
$$The formula shows the 7 parameters of the seasonal trend model with ARMA (1,4) errors that we want to estimate.

Bonus: $\phi_1$ denotes the AR(1) coefficient of model 2. In our model, we assume that errors are not independent (A2), but autocorrelated as we have found evidence for under 1.2.4. The AR(1) coefficient $\phi_1$ is multiplied by the error, we made when predicting $Y_{t-1}$. In other words, it is the impact that the error we made when predicting yesterday's value has on predicting today's value. An AR(1) process is stationary if $|\phi| < 1$. If $\phi = 0$, $u_{t-1}$ has no impact on $Y_t$. If $\phi = 1$, the process results in a random walk. For our $\phi_1$, we expect a stationary process, because the plot of the residuals under 1.2.4 showed evidence for mean reversion and is therefore not a random walk.

### 1.2.8

```{r}
md2 <- Arima(ts_gdp, order=c(4,0,4), xreg=m, fixed=res[which.min(res$AIC),-c(1,2)], method="ML")
#summary(md2)
```

+-----------+------------+-------------+
| Parameter | Model 1    | Model 2     |
+===========+============+=============+
| intercept | 3474919.6\ | 3500269.84\ |
|           | (21981.5)  | (50120.37)  |
+-----------+------------+-------------+
| trend     | 44233.8\   | 44117.005\  |
|           | (737.2)    | (1944.662)  |
+-----------+------------+-------------+
| season2   | 121102.9\  | -60010.12\  |
|           | (23968.7)  | (14153.12)  |
+-----------+------------+-------------+
| season3   | 134742.2\  | 134371.10\  |
|           | (24002.7)  | (14133.86)  |
+-----------+------------+-------------+
| season4   | 178828.6\  | 181863.13\  |
|           | (24059.2)  | (12563.14)  |
+-----------+------------+-------------+
| ar1       | \-         | 0.7970\     |
|           |            | (0.1014)    |
+-----------+------------+-------------+
| ma4       | \-         | 0.8354\     |
|           |            | (0.1921)    |
+-----------+------------+-------------+

: Model estimate comparison (s.e. in parentheses)

### 1.2.9

The intercept of Model 2 is lower than in Model 1, but the trend has changed only a small amount. The standard errors of the seasonal parameters have decreased almost by half in Model 2, but have increased for the intercept and trend parameter. Since we have found under 1.2.4 that the assumption of independent errors were violated, the estimators \$\\beta_0,...,\\beta_k\$ were still consistent (assuming that Model 2 is the more accurate model), but the standard errors were not valid.

Most notably however, the sign of the season2 parameter has changed and is now negative, while the other seasonal parameters have not changed as much. Our best guess is, that the season2 parameter is somehow compensating for the effect of the ARMA(1,4)-model errors.

### 1.2.10

```{r}
plot(ts_gdp, main = "Fig. 5: Training data and model 1/2 fitted values", ylab = "in millions of Dollars", xlab = "Year")
lines(md1$fitted.values, col="red")
lines(md2$fitted, col="green")
legend("bottomright", legend=c("Traning data", "Fitted values (M1)", "Fitted values (M2)"), col=c("black", "red", "green"), lty=1:1)
```

In the above plot, we see the fitted values for Models 1 (red) and 2 (green) along the training data (black). We can see that Model 2 performs better, as it seems to track closer the line of the training data. Model 1 clearly deviates from the training data close in 2010, 2016 and 2019, while Model 2 has almost no visible deviations.

### 1.2.11

```{r}
plot(md2$residuals, main = "Fig. 6: Model 2 residuals", ylab = "residuals", xlab = "Year", col="red")
abline(0,0, col="blue")
plot(md2$residuals^2, main = "Fig. 7: Model 2 squared residuals", ylab = "squared residuals", xlab = "Year", col="red")
abline(0,0, col="blue")

```

By glancing at the plots above, we can see that the residuals now look a lot less correlated and fluctuate regularly above and below 0 for all values of X. Assumption A1 is therefore given. Arguably, A2 is still somewhat violated by the strong outlier in 2012, but nonetheless, the model is an clear improvement over Model 1 overall.

The ACF and PACF show not hint of autocorrelated residuals.

```{r}
Acf(md2$residuals, main = "Fig. 8: Model 2 ACF")
Pacf(md2$residuals, main = "Fig. 9: Model 2 PACF")
```

\newpage

```{r}
Box.test(resid(md2), type = "Ljung-Box", lag=1)
Box.test(resid(md2), type = "Ljung-Box", lag=2)
Box.test(resid(md2), type = "Ljung-Box", lag=3)
Box.test(resid(md2), type = "Ljung-Box", lag=4)
Box.test(resid(md2), type = "Ljung-Box", lag=5)
```

The Box-Ljung test shows no evidence to reject the null hypothesis that there is no autocorrelation up to lag 5.

\newpage

```{r}
hist(md2$residuals, main="Fig. 10: Histogram of Model 2 residuals")
tseries::jarque.bera.test(md2$residuals)
```

Finally, although the histogram looks slightly asymmetrically, the Jarque-Bera-Test gives us no evidence for non-normality in the distribution of residuals.

### 1.2.12

a)  For the first quarter of 2020, we use the estimates of our Model 1 as follows:

```{r, echo=TRUE}
t = nrow(gdp)-4 # the number of quarters in the dataset minus the 4 quarters of 2020
3474919.61 + 44233.75*(t+1)
```

The output of the `forecast()` function confirms our calculation:

```{r, echo=TRUE}
forecast(md1, h=1)
```

\newpage

b)  Bonus: For the first quarter of 2020, we use the estimates of our Model 2 as follows:

```{r, echo=TRUE}
ut1 <- md2$residuals[length(md2$residuals)]
3500269.84 + 44117.005*(t+1) + 0.7970 * ut1
```

We used the residual of Q4 2019 for our error term $u_{t-1}$ and omitted the MA part.

### 1.2.13

```{r}
actual <- window( ts(gdp$NA000334Q, frequency = 4, start = c(2010, 1)), start=c(2020, 1), end=c(2020, 4));
plot(forecast(md1, x=X, h=8))
lines(actual, col="red") 
```

```{r}
X <- cbind(seq(from=41, to=48, by=1), rep(c(0,1,0,0), 2), rep(c(0,0,1,0), 2), rep(c(0,1,0,1), 2))
plot(forecast(md2, x=X, h=8))
lines(actual, col="red")

```

## 2.1

a)  Answer: FALSE

First we can see following: In this process the phi is smaller than 1 which is a first sign that the process might be stationary. A phi of 0.9 is quite close to one and we expect correlation between neighboring values and that values need more time to revert back to the mean. We assume that due to stationarity the mean is constant over time. So $E(Y_{t})= E(Y_{t-1})$ should hold. The given formula is: $Y_{t}=1+\sqrt(t)+0.9Y_{t-1}+u_t$ with $u_t$ \~ $N(0, \sigma^2_u)$ "Proof" of expectation: $$
\begin{aligned}
E(Y_{t})&=E(1+\sqrt{t}+\phi Y_{t-1 }+u_t)\\
&=E(1)+E(\sqrt{t})+E(\phi Y_{t-1})+E(u_t)\\
\end{aligned}
$$

$u_t$ is a random noise process and we know that the expectation of a random noise process is 0, so $E(u_t)= 0$. For the assumption of same expected values for $Y_t$ and $Y_{t-1}$ to hold, $E(\sqrt(t))+1$ need to be zero, which cannot be the case as $\sqrt(t)$ can not be a negative number. In this case, the mean is constantly rising due to $E(\sqrt(t))$. The assumption of a consistent mean is violated and it is not a stationary process.

```{r, echo=TRUE}
n <- 600
u <- rnorm(n, 0, 1)

phi <- 0.9
Y_1 <- 200
Y<- rep(Y_1,n)
for (t in 2:n) {
  Y[t] <- 1 + sqrt(t) + phi*Y[t-1] + u[t]
}
Y <- ts(Y, frequency = 1)

plot(Y, xlim=c(50, 600))
abline(0,0, col="blue")


```

b)  Answer: FALSE

The given process has the formula: $Y_{t}=\sqrt{t}u_t$ with $u_t$ \~ $N(0,\sigma^2_u)$. Here $\phi= 0$, if we assume that $u_t$ is a white noise process with expected value of 0 independently of $\sqrt{t}$. The expected value of $u_t\sqrt{t}$ is zero too. Therefore the expectation would always be zero and the mean constant over time. $\phi=0$ implies that there is no dependence on neighboring values. However, the variance is not constant because it is changing over time with $\sqrt{t}$. Therefore, the process is not stationary.

```{r, echo=TRUE}
n <- 60000
u <- rnorm(n)

Y<- rep(1,n)
for (t in 2:n) {
  Y[t] <- sqrt(t)*u[t]
}
plot(Y, type="l", xlim=c(50, n))
abline(0,0, col="blue")

```

\newpage

c)  Answer: FALSE

The given process $Z_t=\frac{Y_t}{\sqrt(t)}$ where $Y_t=Y_{t-1}+u_t$ with $u_t$ \~ $N(0, \sigma^2_u)$ is not weakly stationary. Something can be called weakly stationary if the distribution is the same over time.

$Y_t=Y_{t-1}+u_t$ is a random walk with $\phi=1$. If $\phi=1$ this implies a random walk and all random walk processes are non-stationary. Calculating the expected value it is shown that it is rising with time t and is not constant. As result one can say it is not weakly stationary.

```{r, echo=TRUE}
n <- 60
u <- rnorm(n)
Z <- rep(1,n)
Y_1 <- 100
Y<- rep(Y_1,n)
for (t in 2:n) {
  Y[t] <- Y[t-1]*u[t]
  Z[t] <- Y[t]/sqrt(t)
}
plot(Z, type="l")
abline(0,0, col="blue")

```

## 2.2

Real world example:

AR-Process: Assuming that some new buyers or sellers of stocks are influenced by recent (past) market transactions when deciding how much to offer or accept for the security, the stock market could be modeled as an AR-process.

MA-Process: A lake's temperature. Shocks are daily variations in temperatures.
